import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.cluster.hierarchy import linkage, dendrogram, fcluster
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from sklearn.preprocessing import StandardScaler, PowerTransformer
from sklearn.datasets import load_iris
import kagglehub
import sys

sys.setrecursionlimit(10000)

# -----------------------
# 1) CARREGANDO OS DADOS
# -----------------------
iris = load_iris()
df_iris = pd.DataFrame(iris.data, columns=iris.feature_names)

path = kagglehub.dataset_download("tmdb/tmdb-movie-metadata")
df_kaggle_original = pd.read_csv(f"{path}/tmdb_5000_movies.csv")
df_kaggle_original = df_kaggle_original[['budget', 'popularity', 'revenue', 'vote_average', 'vote_count']].dropna()

# --------------------------------------------------------
# 2) VISUALIZA√á√ÉO EM PARES (SEM NORMALIZA√á√ÉO)
# --------------------------------------------------------
sns.pairplot(df_iris)
plt.suptitle("Iris - Pairplot (Sem Normaliza√ß√£o)", y=1.02)
plt.show()

sns.pairplot(df_kaggle_original)
plt.suptitle("Kaggle (Original) - Pairplot (Sem Normaliza√ß√£o)", y=1.02)
plt.show()

# --------------------------------------------------------
# 3) NORMALIZA√á√ÉO DAS BASES (STANDARD SCALER)
# --------------------------------------------------------
scaler_iris = StandardScaler()
iris_scaled = scaler_iris.fit_transform(df_iris)
df_iris_scaled = pd.DataFrame(iris_scaled, columns=df_iris.columns)

scaler_kaggle = StandardScaler()
kaggle_scaled = scaler_kaggle.fit_transform(df_kaggle_original)
df_kaggle_scaled = pd.DataFrame(kaggle_scaled, columns=df_kaggle_original.columns)

# --------------------------------------------------------
# 4) POWERTRANSFORMER APENAS NO KAGGLE
# --------------------------------------------------------
pt = PowerTransformer(method='yeo-johnson')
kaggle_transformed = pt.fit_transform(df_kaggle_scaled)
df_kaggle_transformed = pd.DataFrame(kaggle_transformed, columns=df_kaggle_original.columns)

# -------------------------
# PLOTS COM NORMALIZA√á√ÉO
# -------------------------
sns.pairplot(df_iris_scaled)
plt.suptitle("Iris - Pairplot (Apenas Normalizado)", y=1.02)
plt.show()

sns.pairplot(df_kaggle_scaled)
plt.suptitle("Kaggle - Pairplot (Apenas Normalizado)", y=1.02)
plt.show()

sns.pairplot(df_kaggle_transformed)
plt.suptitle("Kaggle - Pairplot (StandardScaler + PowerTransformer)", y=1.02)
plt.show()

# --------------------------------------------------------
# 5) ELBOW METHOD PARA DEFINIR K (3 CASOS)
# --------------------------------------------------------
def elbow_method(data, title):
    wcss = []
    for k in range(1, 11):
        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
        kmeans.fit(data)
        wcss.append(kmeans.inertia_)
    plt.figure(figsize=(5, 4))
    plt.plot(range(1, 11), wcss, marker='o')
    plt.title(f"Elbow - {title}")
    plt.xlabel('N√∫mero de Clusters (K)')
    plt.ylabel('WCSS')
    plt.show()

elbow_method(iris_scaled, "Iris (Normalizado)")
elbow_method(kaggle_scaled, "Kaggle (Apenas Normalizado)")
elbow_method(kaggle_transformed, "Kaggle (Normalizado + PowerTransformer)")

print("Analise os gr√°ficos Elbow e escolha K para cada base.")
k_iris = int(input("Escolha K para IRIS: "))
k_kaggle = int(input("Escolha K para KAGGLE: "))

# --------------------------------------------------------
# 6) K-MEANS
# --------------------------------------------------------
kmeans_iris = KMeans(n_clusters=k_iris, random_state=42, n_init=10)
iris_clusters_kmeans = kmeans_iris.fit_predict(iris_scaled)

kmeans_kaggle_scaled = KMeans(n_clusters=k_kaggle, random_state=42, n_init=10)
kaggle_scaled_clusters_kmeans = kmeans_kaggle_scaled.fit_predict(kaggle_scaled)

kmeans_kaggle_transformed = KMeans(n_clusters=k_kaggle, random_state=42, n_init=10)
kaggle_transformed_clusters_kmeans = kmeans_kaggle_transformed.fit_predict(kaggle_transformed)

# --------------------------------------------------------
# 7) HIERARQUICO
# --------------------------------------------------------
def hierarchical_clustering(data, title, k, method):
    linked = linkage(data, method=method)
    plt.figure(figsize=(10, 5))
    dendrogram(linked,
               truncate_mode='lastp',  # mostra s√≥ os √∫ltimos clusters
               p=50,
               show_leaf_counts=True,
               leaf_rotation=90,
               leaf_font_size=10,
               show_contracted=True)
    plt.title(f"Dendrograma Resumido ({method}) - {title}")
    plt.xlabel('Clusters')
    plt.ylabel('Dist√¢ncia')
    plt.tight_layout()
    plt.show()
    return fcluster(linked, t=k, criterion='maxclust')

# Ward method
iris_clusters_hier_ward = hierarchical_clustering(iris_scaled, "Iris (Normalizado)", k_iris, 'ward')
kaggle_scaled_clusters_hier_ward = hierarchical_clustering(kaggle_scaled, "Kaggle (Normalizado)", k_kaggle, 'ward')
kaggle_transformed_clusters_hier_ward = hierarchical_clustering(kaggle_transformed, "Kaggle (Transformado)", k_kaggle, 'ward')

# Single method
iris_clusters_hier_single = hierarchical_clustering(iris_scaled, "Iris (Normalizado)", k_iris, 'single')
kaggle_scaled_clusters_hier_single = hierarchical_clustering(kaggle_scaled, "Kaggle (Normalizado)", k_kaggle, 'single')
kaggle_transformed_clusters_hier_single = hierarchical_clustering(kaggle_transformed, "Kaggle (Transformado)", k_kaggle, 'single')

# Complete method
iris_clusters_hier_complete = hierarchical_clustering(iris_scaled, "Iris (Normalizado)", k_iris, 'complete')
kaggle_scaled_clusters_hier_complete = hierarchical_clustering(kaggle_scaled, "Kaggle (Normalizado)", k_kaggle, 'complete')
kaggle_transformed_clusters_hier_complete = hierarchical_clustering(kaggle_transformed, "Kaggle (Transformado)", k_kaggle, 'complete')

# Average method
iris_clusters_hier_average = hierarchical_clustering(iris_scaled, "Iris (Normalizado)", k_iris, 'average')
kaggle_scaled_clusters_hier_average = hierarchical_clustering(kaggle_scaled, "Kaggle (Normalizado)", k_kaggle, 'average')
kaggle_transformed_clusters_hier_average = hierarchical_clustering(kaggle_transformed, "Kaggle (Transformado)", k_kaggle, 'average')
# --------------------------------------------------------
# 8) SILHOUETTE SCORE - TODOS OS M√âTODOS
# --------------------------------------------------------

# ===== IRIS (normalizada) =====
score_kmeans_iris = silhouette_score(iris_scaled, iris_clusters_kmeans)
score_hier_iris_ward = silhouette_score(iris_scaled, iris_clusters_hier_ward)
score_hier_iris_single = silhouette_score(iris_scaled, iris_clusters_hier_single)
score_hier_iris_complete = silhouette_score(iris_scaled, iris_clusters_hier_complete)
score_hier_iris_average = silhouette_score(iris_scaled, iris_clusters_hier_average)

# ===== KAGGLE (apenas normalizada) =====
score_kmeans_kaggle_scaled = silhouette_score(kaggle_scaled, kaggle_scaled_clusters_kmeans)
score_hier_kaggle_scaled_ward = silhouette_score(kaggle_scaled, kaggle_scaled_clusters_hier_ward)
score_hier_kaggle_scaled_single = silhouette_score(kaggle_scaled, kaggle_scaled_clusters_hier_single)
score_hier_kaggle_scaled_complete = silhouette_score(kaggle_scaled, kaggle_scaled_clusters_hier_complete)
score_hier_kaggle_scaled_average = silhouette_score(kaggle_scaled, kaggle_scaled_clusters_hier_average)

# ===== KAGGLE (normalizada + PowerTransformer) =====
score_kmeans_kaggle_transf = silhouette_score(kaggle_transformed, kaggle_transformed_clusters_kmeans)
score_hier_kaggle_transf_ward = silhouette_score(kaggle_transformed, kaggle_transformed_clusters_hier_ward)
score_hier_kaggle_transf_single = silhouette_score(kaggle_transformed, kaggle_transformed_clusters_hier_single)
score_hier_kaggle_transf_complete = silhouette_score(kaggle_transformed, kaggle_transformed_clusters_hier_complete)
score_hier_kaggle_transf_average = silhouette_score(kaggle_transformed, kaggle_transformed_clusters_hier_average)

# --------------------------------------------------------
# PRINT DOS RESULTADOS
# --------------------------------------------------------

print("\n===== COMPARATIVO SILHOUETTE SCORE =====\n")

print("üìå Base IRIS (apenas normalizada):")
print(f"  ‚Ä¢ K-Means:              {score_kmeans_iris:.4f}")
print(f"  ‚Ä¢ Hier√°rquico (Ward):   {score_hier_iris_ward:.4f}")
print(f"  ‚Ä¢ Hier√°rquico (Single): {score_hier_iris_single:.4f}")
print(f"  ‚Ä¢ Hier√°rquico (Complete): {score_hier_iris_complete:.4f}")
print(f"  ‚Ä¢ Hier√°rquico (Average):  {score_hier_iris_average:.4f}\n")

print("üìå Base KAGGLE (apenas normalizada):")
print(f"  ‚Ä¢ K-Means:              {score_kmeans_kaggle_scaled:.4f}")
print(f"  ‚Ä¢ Hier√°rquico (Ward):   {score_hier_kaggle_scaled_ward:.4f}")
print(f"  ‚Ä¢ Hier√°rquico (Single): {score_hier_kaggle_scaled_single:.4f}")
print(f"  ‚Ä¢ Hier√°rquico (Complete): {score_hier_kaggle_scaled_complete:.4f}")
print(f"  ‚Ä¢ Hier√°rquico (Average):  {score_hier_kaggle_scaled_average:.4f}\n")

print("üìå Base KAGGLE (StandardScaler + PowerTransformer):")
print(f"  ‚Ä¢ K-Means:              {score_kmeans_kaggle_transf:.4f}")
print(f"  ‚Ä¢ Hier√°rquico (Ward):   {score_hier_kaggle_transf_ward:.4f}")
print(f"  ‚Ä¢ Hier√°rquico (Single): {score_hier_kaggle_transf_single:.4f}")
print(f"  ‚Ä¢ Hier√°rquico (Complete): {score_hier_kaggle_transf_complete:.4f}")
print(f"  ‚Ä¢ Hier√°rquico (Average):  {score_hier_kaggle_transf_average:.4f}")

# --------------------------------------------------------
# RELAT√ìRIO FINAL (DETALHES, ESCOLHAS E JUSTIFICATIVAS)
# --------------------------------------------------------
'''
RELAT√ìRIO FINAL (VERS√ÉO APROFUNDADA):

1) DATASETS E DIMENS√ïES:

   a) Base Iris:
      - Possui 4 dimens√µes: sepal length, sepal width, petal length e petal width.
      - S√£o 150 inst√¢ncias (amostras), normalmente classificadas em 3 esp√©cies de flor (Setosa, Versicolor, Virginica).
      - Aqui, o objetivo √© clustering. isso significa que estamos fazendo Aprendizado de M√°quina N√£o Supervisionado, ou seja,
        n√£o usamos os r√≥tulos originais (as esp√©cies) para treinar, mas apenas para comparar ou avaliar depois, se quisermos.

   b) Base Kaggle (TMDB):
      - Selecionamos 5 dimens√µes num√©ricas: budget, popularity, revenue, vote_average e vote_count.
      - O m√©todo dropna() remove linhas com valores ausentes (NaNs). Isso garante que s√≥ tenhamos dados completos para cada coluna.
      - Escolhemos essas 5 dimens√µes porque s√£o vari√°veis num√©ricas relacionadas ao desempenho e m√©tricas de filmes
        (ex.: dinheiro investido, receita, popularidade e avalia√ß√µes). Outras colunas poderiam estar muito correlacionadas
        ou sem relev√¢ncia num√©rica, ent√£o optamos por essas 5 para simplificar o modelo.

2) POR QUE NORMALIZAMOS?

   - Tanto em Iris quanto em Kaggle, usamos algoritmos de clustering baseados em dist√¢ncia Euclidiana (muitos teoremas de pit√°goras...), como o K-Means e o Ward.
   - A dist√¢ncia Euclidiana calcula a "hipotenusa" entre pontos no espa√ßo n-dimensional:
       d(p, q) = sqrt( (p1 - q1)^2 + (p2 - q2)^2 + ... + (pn - qn)^2 )
   - Se uma vari√°vel tem valores muito maiores que as outras (por exemplo, budget em milh√µes vs. vote_average em 0~10), ela domina o c√°lculo de dist√¢ncia.
   - Para evitar isso, aplicamos StandardScaler, que transforma cada coluna para ter m√©dia 0 e desvio padr√£o 1, deixando-as com ‚Äúpeso‚Äù equilibrado.
   - O StandardScaler √© fornecido pela biblioteca scikit-learn (sklearn.preprocessing). Ele √© comum em pipelines de machine learning.
   - Apesar das vari√°veis do Iris estarem em escalas parecidas (todas em cent√≠metros), **a diferen√ßa entre seus desvios padr√µes ainda pode distorcer dist√¢ncias**,
    o que afeta negativamente o clustering. Por isso, normalizamos tamb√©m (petal width varia muito menos que petal length, 2.5cm para 7cm).

3) POR QUE APLICAMOS POWERTRANSFORMER NO KAGGLE?

   - Mesmo ap√≥s normalizar (StandardScaler), as vari√°veis budget e revenue, por exemplo, mant√™m distribui√ß√µes muito assim√©tricas (skewed).
   - Skewness (assimetria) indica que a maior parte dos valores se concentra em uma faixa pequena, enquanto h√° caudas muito longas em um lado.
   - O PowerTransformer, tamb√©m da biblioteca scikit-learn, m√©todo 'yeo-johnson', tenta corrigir essa assimetria aplicando transforma√ß√µes matem√°ticas
     em cada coluna, aproximando-a de uma distribui√ß√£o Gaussiana (ou "normal").
   - Gaussiana (ou distribui√ß√£o normal) √© aquela curva em formato de sino (bell curve), centrada na m√©dia, com simetria e uma vari√¢ncia bem definida.
   - Algoritmos como K-Means e Ward ‚Äúassumem‚Äù clusters mais ou menos esf√©ricos; se os dados estiverem muito alongados ou com outliers extremos,
     o centr√≥ide ou a vari√¢ncia intracluster fica distorcida.
   - Escolhemos 'yeo-johnson' porque ele funciona com valores negativos, o que pode ocorrer ap√≥s a normaliza√ß√£o. 
     O m√©todo 'box-cox' foi descartado porque s√≥ funciona com dados estritamente positivos.
     ‚Ä¢ PowerTransformer (Yeo-Johnson) ‚Äî √∫til para reduzir skewness e tornar os dados mais gaussianos, mesmo com valores negativos. Aplica transforma√ß√µes logar√≠tmicas ou de pot√™ncia.
     ‚Ä¢ PowerTransformer (Box-Cox) ‚Äî tamb√©m reduz skewness, similar ao yeo, mas s√≥ funciona com valores estritamente positivos. 
   - Outros m√©todos que poderiam ser considerados:
     ‚Ä¢ Log1p (log(1 + x)) ‚Äî simples, mas n√£o corrige bem valores negativos.
     ‚Ä¢ QuantileTransformer ‚Äî for√ßa os dados a uma distribui√ß√£o desejada, mas distorce rela√ß√µes locais.
     ‚Ä¢ RobustScaler ‚Äî √∫til para outliers, mas n√£o resolve skewness.
   - O Yeo-Johnson foi a melhor escolha aqui por ser vers√°til, eficaz e autom√°tico para cada coluna.

4) M√âTODO DO JOELHO (ELBOW METHOD):

   - O Elbow Method √© usado para sugerir o n√∫mero adequado de clusters (K) num algoritmo como K-Means.
   - Calcula-se a soma das dist√¢ncias (WCSS) de cada ponto ao seu centr√≥ide em fun√ß√£o de K. Normalmente, conforme K cresce, o WCSS diminui,
     mas tende a ‚Äúestabilizar‚Äù ap√≥s certo ponto. Esse ponto de ‚Äúcurva‚Äù ou ‚Äújoelho‚Äù indica um bom equil√≠brio entre coes√£o intracluster e complexidade.

5) ALGORITMOS DE AGRUPAMENTO:

   - K-Means: biblioteca scikit-learn (sklearn.cluster.KMeans).
     - Ele aceita par√¢metros como:
       n_clusters (quantidade de grupos),
       init (m√©todo de inicializa√ß√£o dos centr√≥ides),
       n_init (quantas vezes ele repete o processo para evitar m√≠nimo local),
       random_state (para reprodutibilidade) e outros.
     - Escolhemos n_init=10 para rodar v√°rias inicializa√ß√µes e pegar o melhor resultado
       (evitando cair em um centr√≥ide ruim).
     - random_state=42 √© s√≥ para garantir que o resultado seja reproduz√≠vel em diferentes execu√ß√µes.

   - Hierarchical Clustering (Ward):
     - Usamos linkage='ward', que minimiza a vari√¢ncia dentro de cada cluster a cada fus√£o.
     ‚Ä¢ Optamos pelo m√©todo 'ward' porque ele minimiza a vari√¢ncia intracluster a cada fus√£o,
       garantindo que os clusters formados sejam mais compactos e homog√™neos internamente.
       Ele calcula a soma dos quadrados das diferen√ßas dentro dos grupos (similar ao K-Means),
       tornando os resultados mais consistentes e compar√°veis.
     ‚Ä¢ Esse m√©todo funciona especialmente bem em dados que foram normalizados e transformados
     para se aproximarem de uma distribui√ß√£o gaussiana ‚Äî exatamente o caso do nosso pipeline
     com StandardScaler e PowerTransformer.
     ‚Ä¢ Comparando com outros m√©todos:
       - 'single' usa a menor dist√¢ncia entre pontos, mas √© muito sens√≠vel a outliers e tende
          a formar clusters "esticados" ou encadeados.
       - 'complete' considera a maior dist√¢ncia entre pontos, o que pode exagerar a separa√ß√£o
          e formar clusters pequenos e distantes.
       - 'average' usa a m√©dia das dist√¢ncias entre todos os pares, mas n√£o leva em conta a
          vari√¢ncia interna como o 'ward' faz.
          
6) SILHOUETTE SCORE:

   - √â uma m√©trica interna de valida√ß√£o de clusters, tamb√©m disponibilizada pelo scikit-learn (sklearn.metrics.silhouette_score).
   - Para cada ponto, mede a coes√£o com seu cluster e a separa√ß√£o em rela√ß√£o aos outros clusters, gerando valores entre -1 e +1.
   - Quanto mais pr√≥ximo de +1, mais satisfat√≥ria a separa√ß√£o; valores negativos sugerem pontos ‚Äúmal clusterizados‚Äù.
   - Usamos para comparar rapidamente a qualidade dos grupos gerados por K-Means e Hierarchical nas diferentes vers√µes: ‚Äúsem transformar‚Äù, ‚Äús√≥ normalizar‚Äù e ‚Äúnormalizar + PowerTransformer‚Äù.

CONCLUS√ÉO SOBRE O PROJETO E ATIVIDADE:

- O projeto aplicou dois algoritmos (K-Means e Hier√°rquico) em duas bases (Iris e Kaggle).
- Em Iris, normaliza√ß√£o j√° √© suficiente devido √†s escalas moderadamente diferentes (4 ~ 8 cm vs. 0.1 ~ 2.5 cm).
- Em Kaggle, as diferen√ßas de escalas e a skewness das vari√°veis levaram a uma grande melhora ap√≥s usar ‚ÄúStandardScaler + PowerTransformer‚Äù.
- O Elbow Method ajudou a escolher K, e a m√©trica Silhouette Score validou a qualidade das parti√ß√µes.
- Ward foi preferido no hier√°rquico por seu crit√©rio de minimiza√ß√£o da vari√¢ncia, gerando clusters mais coesos.
- Tudo se encaixa nos requisitos da atividade: experimentamos diferentes par√¢metros, discutimos resultados, e apresentamos gr√°ficos e m√©tricas para confirmar as escolhas.

- AGORA SO FALTA COLOCAR PRINTS NO RELATORIO FINAL E DESENVOLVER UM POUCO O TEXTO MAS A IDEIA ESTA AI, DESENVOLVER CADA DETALHE
'''

