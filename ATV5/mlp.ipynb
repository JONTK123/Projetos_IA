{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import kagglehub\n",
    "import pandas as pd\n",
    "\n",
    "path = kagglehub.dataset_download(\"sulianova/cardiovascular-disease-dataset\")\n",
    "df = pd.read_csv(f\"{path}/cardio_train.csv\", sep=';')\n",
    "\n",
    "df = df.drop(columns=[\"id\"], errors=\"ignore\")\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "df_limpo = df[\n",
    "    (df[\"age\"] >= 10000) & (df[\"age\"] <= 30000) &\n",
    "    (df[\"height\"] >= 120) & (df[\"height\"] <= 220) &\n",
    "    (df[\"weight\"] >= 30) & (df[\"weight\"] <= 200) &\n",
    "    (df[\"ap_hi\"] >= 90) & (df[\"ap_hi\"] <= 250) &\n",
    "    (df[\"ap_lo\"] >= 60) & (df[\"ap_lo\"] <= 150)\n",
    "].copy()\n",
    "\n",
    "df_limpo[\"age_years\"] = (df_limpo[\"age\"] / 365.25).astype(int)\n",
    "\n",
    "X = df_limpo.drop(columns=[\"cardio\"])\n",
    "y = df_limpo[\"cardio\"]\n",
    "\n",
    "provavel_categoricas = [\"gender\", \"cholesterol\", \"gluc\", \"smoke\", \"alco\", \"active\"]\n",
    "\n",
    "features_all = X.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
    "\n",
    "features_bin = [col for col in features_all if col in provavel_categoricas]\n",
    "features_cont = [col for col in features_all if col not in provavel_categoricas]\n",
    "\n",
    "print(\", \".join(features_cont))\n",
    "print(\", \".join(features_bin))\n",
    "\n",
    "print(\"age_years   -> Idade em anos (convertida de 'age' em dias)\")\n",
    "print(\"height      -> Altura em centímetros\")\n",
    "print(\"weight      -> Peso em quilogramas\")\n",
    "print(\"ap_hi       -> Pressão arterial sistólica (máxima)\")\n",
    "print(\"ap_lo       -> Pressão arterial diastólica (mínima)\")\n",
    "\n",
    "print(\"gender      -> Sexo: 1 = homem, 2 = mulher\")\n",
    "print(\"cholesterol -> Nível de colesterol: 1 = normal, 2 = alto, 3 = muito alto\")\n",
    "print(\"gluc        -> Nível de glicose: 1 = normal, 2 = alto, 3 = muito alto\")\n",
    "print(\"smoke       -> Fumante: 0 = não, 1 = sim\")\n",
    "print(\"alco        -> Consome álcool: 0 = não, 1 = sim\")\n",
    "print(\"active      -> Fisicamente ativo: 0 = não, 1 = sim\")\n",
    "\n",
    "print(\"cardio      -> Presença de doença cardiovascular: 0 = não, 1 = sim\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "print(\"=== Bloco de Análise Exploratória ===\\n\")\n",
    "\n",
    "features_cont = [col for col in features_cont if col != \"age\"]\n",
    "\n",
    "# 1. Histogramas das variáveis contínuas por classe\n",
    "print(\"\\n=== 1. Histogramas das variáveis contínuas por classe ===\")\n",
    "for feature in features_cont:\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    sns.histplot(data=df_limpo, x=feature, hue='cardio', kde=True, bins=30, palette='Set1', element='step')\n",
    "    plt.title(f'Distribuição de {feature} por Classe (cardio)')\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel('Frequência')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 2. Countplot de Faixa Etária por Classe\n",
    "print(\"\\n=== 2. Distribuição de Faixa Etária por Classe ===\")\n",
    "df_limpo[\"faixa_etaria\"] = pd.cut(\n",
    "    df_limpo[\"age_years\"],\n",
    "    bins=[20, 30, 40, 50, 60, 70],\n",
    "    labels=[\"20-30\", \"31-40\", \"41-50\", \"51-60\", \"61-70\"]\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.countplot(data=df_limpo, x='faixa_etaria', hue='cardio', palette='Set1')\n",
    "plt.title('Distribuição por Faixa Etária e Presença de Doença Cardíaca')\n",
    "plt.xlabel('Faixa Etária')\n",
    "plt.ylabel('Contagem')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 3. Countplot de ATIVO (active)\n",
    "print(\"\\n=== 3. Nível de Atividade Física por Classe ===\")\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.countplot(data=df_limpo, x='active', hue='cardio', palette='Set1')\n",
    "plt.title('Atividade Física vs Doença Cardíaca')\n",
    "plt.xlabel('Ativo (0 = Não, 1 = Sim)')\n",
    "plt.ylabel('Contagem')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 4. Countplot de FUMANTE (smoke)\n",
    "print(\"\\n=== 4. Fumante por Classe ===\")\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.countplot(data=df_limpo, x='smoke', hue='cardio', palette='Set1')\n",
    "plt.title('Fumante vs Doença Cardíaca')\n",
    "plt.xlabel('Fumante (0 = Não, 1 = Sim)')\n",
    "plt.ylabel('Contagem')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 5. Pairplot entre variáveis contínuas\n",
    "print(\"\\n=== 5. Dispersão entre variáveis contínuas (pairplot) ===\")\n",
    "sns.pairplot(\n",
    "    data=df_limpo,\n",
    "    vars=features_cont,\n",
    "    hue='cardio',\n",
    "    palette='Set1',\n",
    "    diag_kind='kde',\n",
    "    plot_kws={'alpha': 0.6}\n",
    ")\n",
    "plt.suptitle(\"Pairplot das Variáveis Contínuas por Classe (cardio)\", y=1.02)\n",
    "plt.show()\n",
    "\n",
    "# 6. PCA com variáveis CONTÍNUAS (com normalização)\n",
    "print(\"\\n=== 6. PCA com Variáveis CONTÍNUAS (normalizadas) ===\")\n",
    "scaler_cont = StandardScaler()\n",
    "X_cont_scaled = scaler_cont.fit_transform(df_limpo[features_cont])\n",
    "\n",
    "pca_cont = PCA(n_components=2)\n",
    "X_pca_cont = pca_cont.fit_transform(X_cont_scaled)\n",
    "\n",
    "plt.figure(figsize=(7, 5))\n",
    "sns.scatterplot(x=X_pca_cont[:, 0], y=X_pca_cont[:, 1], hue=y, palette='Set1', alpha=0.7)\n",
    "plt.title(\"PCA - Variáveis CONTÍNUAS (normalizadas)\")\n",
    "plt.xlabel(\"Componente Principal 1\")\n",
    "plt.ylabel(\"Componente Principal 2\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 7. PCA com variáveis CONTÍNUAS (normalizadas) + BINÁRIAS/CATEGÓRICAS (sem normalizar)\n",
    "print(\"\\n=== 7. PCA com Variáveis CONTÍNUAS (normalizadas) + BINÁRIAS/CATEGÓRICAS ===\")\n",
    "\n",
    "scaler_cont = StandardScaler()\n",
    "X_cont_scaled = scaler_cont.fit_transform(df_limpo[features_cont])\n",
    "X_cont_df = pd.DataFrame(X_cont_scaled, columns=features_cont, index=df_limpo.index)\n",
    "\n",
    "X_combined = pd.concat([X_cont_df, df_limpo[features_bin].astype(float)], axis=1)\n",
    "\n",
    "pca_combined = PCA(n_components=2)\n",
    "X_pca_combined = pca_combined.fit_transform(X_combined)\n",
    "\n",
    "plt.figure(figsize=(7, 5))\n",
    "sns.scatterplot(x=X_pca_combined[:, 0], y=X_pca_combined[:, 1], hue=y, palette='Set1', alpha=0.7)\n",
    "plt.title(\"PCA - CONTÍNUAS (normalizadas) + BINÁRIAS/CATEGÓRICAS (sem normalizar)\")\n",
    "plt.xlabel(\"Componente Principal 1\")\n",
    "plt.ylabel(\"Componente Principal 2\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "142fe034d6b0933f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Normalização com StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_scaled = df_limpo.copy()\n",
    "X_scaled[features_cont] = scaler.fit_transform(X_scaled[features_cont])\n",
    "\n",
    "print(\"=== Histogramas Antes e Depois da Normalização ===\")\n",
    "\n",
    "# Plot comparativo\n",
    "fig, axes = plt.subplots(len(features_cont), 2, figsize=(12, len(features_cont)*2.5))\n",
    "fig.suptitle('Comparação de Distribuições: Antes vs. Depois da Normalização', fontsize=16)\n",
    "\n",
    "for i, feature in enumerate(features_cont):\n",
    "    axes[i, 0].hist(df_limpo[feature], bins=30, color='blue', alpha=0.7)\n",
    "    axes[i, 0].set_title(f'Original: {feature}')\n",
    "\n",
    "    axes[i, 1].hist(X_scaled[feature], bins=30, color='green', alpha=0.7)\n",
    "    axes[i, 1].set_title(f'Normalizado: {feature}')\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "plt.show()\n"
   ],
   "id": "3eed83935c3cc941",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Separar treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_combined, y, test_size=0.20, stratify=y, random_state=42)\n",
    "\n",
    "# 2. Modelo-base\n",
    "mlp_base = MLPClassifier(\n",
    "    solver='adam',\n",
    "    early_stopping=True,\n",
    "    validation_fraction=0.10,\n",
    "    random_state=42,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# 3. Grid de hiperparâmetros\n",
    "param_grid = {\n",
    "    \"hidden_layer_sizes\": [(20,), (50,), (100,), (20,20), (50, 50), (100, 100)],\n",
    "    \"activation\": [\"relu\", \"tanh\", \"logistic\"],\n",
    "    \"learning_rate_init\": [0.001, 0.01, 0.1],\n",
    "    \"max_iter\": [500, 750, 1000],\n",
    "}\n",
    "\n",
    "# 4. GridSearchCV\n",
    "grid = GridSearchCV(\n",
    "    estimator=mlp_base,\n",
    "    param_grid=param_grid,\n",
    "    scoring=\"accuracy\",\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# 5. Treinamento\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# 6. Avaliação e melhores parâmetros\n",
    "print(\"\\nMelhores Hiperparâmetros encontrados pelo GridSearchCV:\")\n",
    "best_params = grid.best_params_\n",
    "for param, valor in best_params.items():\n",
    "    print(f\"• {param}: {valor}\")\n",
    "\n",
    "print(f\"\\nMelhor acurácia média durante a validação cruzada: {grid.best_score_:.4f}\")\n",
    "\n",
    "# 7. Interpretação\n",
    "print(\"\\nInterpretação dos parâmetros escolhidos:\")\n",
    "if 'hidden_layer_sizes' in best_params:\n",
    "    print(f\"• hidden_layer_sizes = {best_params['hidden_layer_sizes']} → Define a estrutura da rede. \"\n",
    "          f\"Camadas como {best_params['hidden_layer_sizes']} indicam capacidade de capturar padrões mais complexos.\")\n",
    "\n",
    "if 'activation' in best_params:\n",
    "    ativ = best_params['activation']\n",
    "    print(f\"• activation = '{ativ}' → \"\n",
    "          + (\"'relu': Rápido e eficaz para não-linearidades.\" if ativ == \"relu\" else\n",
    "             \"'tanh': Bom para dados centrados em 0.\" if ativ == \"tanh\" else\n",
    "             \"'logistic': Simples e tradicional.\"))\n",
    "\n",
    "if 'learning_rate_init' in best_params:\n",
    "    lr = best_params['learning_rate_init']\n",
    "    print(f\"• learning_rate_init = {lr} → Controla a velocidade de aprendizagem. Valor intermediário como {lr} busca estabilidade.\")\n",
    "\n",
    "if 'max_iter' in best_params:\n",
    "    print(f\"• max_iter = {best_params['max_iter']} → Número máximo de épocas. Com early stopping, ajuda a evitar overfitting.\")\n",
    "\n",
    "# 8. Top 5 melhores combinações\n",
    "print(\"\\nTop 5 combinações de hiperparâmetros por acurácia média:\")\n",
    "resultados = pd.DataFrame(grid.cv_results_)\n",
    "top5 = resultados[['mean_test_score', 'params']].sort_values(by='mean_test_score', ascending=False).head(5)\n",
    "\n",
    "for idx, row in enumerate(top5.itertuples(), start=1):\n",
    "    print(f\"\\n#{idx} - Acurácia média: {row.mean_test_score:.4f}\")\n",
    "    for key, val in row.params.items():\n",
    "        print(f\"   → {key}: {val}\")"
   ],
   "id": "fda3035c925f18ef",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# === Avaliação Final do Melhor Modelo (MLP do GridSearch) ===\n",
    "print(\"\\nAvaliação Final - MLP (Melhor Modelo do GridSearch)\\n\")\n",
    "\n",
    "# 1. Obter o melhor estimador\n",
    "best_mlp = grid.best_estimator_\n",
    "\n",
    "# 2. Previsões no conjunto de teste (20%)\n",
    "y_pred_mlp = best_mlp.predict(X_test)\n",
    "\n",
    "# 3. Relatório de Classificação\n",
    "print(\"Classification Report (conjunto de teste):\")\n",
    "print(classification_report(y_test, y_pred_mlp))\n",
    "\n",
    "# 4. Matriz de Confusão\n",
    "cm = confusion_matrix(y_test, y_pred_mlp)\n",
    "print(\"\\nMatriz de Confusão:\")\n",
    "print(cm)\n",
    "\n",
    "# 5. Métricas globais\n",
    "acc = accuracy_score(y_test, y_pred_mlp)\n",
    "f1 = f1_score(y_test, y_pred_mlp, average='macro')\n",
    "print(\"\\nMétricas Globais:\")\n",
    "print(f\"• Acurácia : {acc:.4f}\")\n",
    "print(f\"• F1-Score : {f1:.4f}\")\n",
    "\n",
    "# 6. Heatmap da Matriz de Confusão\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Oranges')\n",
    "plt.title('Matriz de Confusão - MLP (Teste)')\n",
    "plt.xlabel('Classe Predita')\n",
    "plt.ylabel('Classe Real')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 7. Validação Cruzada no conjunto COMPLETO (não recomendada para avaliação final)\n",
    "print(\"\\nValidação Cruzada em TODOS os dados (X e y) [apenas exploratória]:\")\n",
    "cv_scores_all = cross_val_score(best_mlp, X, y, cv=5, scoring='accuracy')\n",
    "print(\"• Acurácias em cada fold:\", cv_scores_all)\n",
    "print(f\"• Acurácia Média (X completo): {cv_scores_all.mean():.4f}\")\n",
    "\n",
    "# 8. Curva de aprendizado (se disponível)\n",
    "if hasattr(best_mlp, \"loss_curve_\"):\n",
    "    print(\"\\nCurva de Aprendizado (Loss por época):\")\n",
    "    plt.figure()\n",
    "    plt.plot(best_mlp.loss_curve_)\n",
    "    plt.title(\"Curva de Aprendizado - MLP\")\n",
    "    plt.xlabel(\"Épocas\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Modelo não possui atributo 'loss_curve_'.\")\n",
    "\n",
    "# 9. Validação Cruzada nos 20% de TESTE\n",
    "print(\"\\nValidação Cruzada (cv=5) no CONJUNTO DE TESTE (20%):\")\n",
    "cv_scores_test = cross_val_score(best_mlp, X_test, y_test, cv=5, scoring='accuracy')\n",
    "print(\"• Acurácias em cada fold (teste):\", cv_scores_test)\n",
    "print(f\"• Acurácia Média (teste): {cv_scores_test.mean():.4f}\")\n",
    "\n",
    "# # Gráfico de Acurácia por Época\n",
    "# plt.figure()\n",
    "# plt.plot(historico_acuracia)\n",
    "# plt.xlabel(\"Época\")\n",
    "# plt.ylabel(\"Acurácia\")\n",
    "# plt.title(\"Acurácia por época\")\n",
    "# plt.grid(True)\n",
    "# plt.savefig(\"grafico_acuracia_mlp.png\")\n",
    "# plt.show()\n",
    "#\n",
    "# # Gráfico 3D da Perda por Batch e Época\n",
    "# fig = plt.figure()\n",
    "# ax = fig.add_subplot(111, projection=\"3d\")\n",
    "#\n",
    "# max_batches = max(len(l) for l in matriz_perda_batches)\n",
    "# X, Y = np.meshgrid(\n",
    "#     np.arange(len(matriz_perda_batches)),\n",
    "#     np.arange(max_batches),\n",
    "# )\n",
    "# Z = np.zeros_like(X, dtype=float)\n",
    "#\n",
    "# for e, losses in enumerate(matriz_perda_batches):\n",
    "#     Z[: len(losses), e] = losses\n",
    "#\n",
    "# ax.plot_surface(X, Y, Z, cmap=\"viridis\")\n",
    "# ax.set_xlabel(\"Época\")\n",
    "# ax.set_ylabel(\"Batch\")\n",
    "# ax.set_zlabel(\"Loss\")\n",
    "# ax.set_title(\"Loss por batch ao longo das épocas\")\n",
    "# plt.savefig(\"grafico_mapa3d_mlp.png\")\n",
    "# plt.show()"
   ],
   "id": "922a65ecb6d915eb",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
